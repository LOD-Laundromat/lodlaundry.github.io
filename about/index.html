
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="SPARQL endpoint of the LOD Laundromat">
    <meta name="author" content="Laurens Rietveld">
    <link rel="icon" href="/imgs/laundry.ico">
    <title>About</title>

    <!-- Bootstrap core CSS -->
    <link href="/bower_components/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="/bower_components/YASQE/dist/yasqe.min.css" rel="stylesheet">
    <link href="/bower_components/YASR/dist/yasr.min.css" rel="stylesheet">
    <link href="/style.css" rel="stylesheet">
    <link href="about.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
      <link rel="stylesheet" href="/bower_components/github-fork-ribbon-css/gh-fork-ribbon.ie.css">
    <![endif]-->
  </head>
  <body>
	  <div id="pageHeader" class="navbar navbar-default navbar-fixed-top bs-docs-nav">
		<div class="container">
			<div class="navbar-header">
			  <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			  </button>
			</div>
			<nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
				<ul id='topNavBar' class="nav navbar-nav">
				</ul>
			</nav>
		</div>
	</div>
    <div class="jumbotron" style="height:100%; position:relative;">
      <div class="container">
        <div class="row">
          <div class="col-md-4">
            <img
                alt="Dev Corner"
                class="mediumLogo"
                src="/imgs/laundryLine.png"
            >
          </div>
          <div class="col-md-8" style="vertical-align: middle;">
            <h1>About</h1>
            <p>
             What exactly do you publish, how do you do it, and what can I do with it?  
            </p>
          </div>
        </div>
      </div>
    </div>
  
      <div class="container">
        <div class="row">
          <div class="col-md-12">
           
            
          	<h2>Code</h2>
          	The part of the LOD Laundromat we use for crawling is called the LOD WashingMachine, and is freely available on <a href="https://github.com/LODLaundry/llWashingMachine" target="_blank">Github</a>.
          	The <a href="https://github.com/LODLaundry" target="_blank">LOD Laundromat GitHub organization</a> contains other interesting repositories as well, from example (fast) parser implementations, to examples on how you might use the LOD Laundromat for analyzing many sets of Linked Data.
          	<h2>Publications</h2>
          	
			<ul>
			<li>Beek, W. & Rietveld, L & Bazoobandi, H.R. & Wielemaker, J. & Schlobach, S.: LOD Laundromat: A Uniform Way of Publishing Other People's Dirty Data. Proceedings of the International Semantic Web Conference (2014). <a class="btn-small" href="/pdf/LOD_Laundromat_-_A_Uniform_Way_of_Publishing_Other_Peoples_Dirty_Data.pdf" target="_blank">pdf</a>
			<div class="well">
It is widely accepted that <i>proper</i> data publishing is difficult.
The majority of Linked Open Data (LOD) does not meet
even a core set of data publishing guidelines.
Moreover, datasets that are <i>clean</i> at creation,
can get <i>stains</i> over time.
As a result, the LOD cloud now contains a high level of <i>dirty</i> data
that is difficult for humans to clean and for machines to process.<br>

Existing solutions for cleaning data (standards, guidelines, tools)
are targeted towards human data creators,
who can (and do) choose not to use them.
This paper presents the LOD Laundromat, which removes stains from data
without any human intervention.
This fully automated approach is able to make very large amounts of LOD
more easily available for further processing <i>right now</i>.<br>

The LOD Laundromat is not a new dataset,
but rather a uniform point of entry to a collection of cleaned siblings
of existing datasets.
It provides researchers and application developers a wealth of data
that is guaranteed to conform to a specified set of best practices,
thereby greatly improving the chance of data actually being (re)used.

</div></li>
			</ul>
            <h2>FAQ</h2>
            <div class="faqItem">
	            <p class="question">What exactly do you publish?</p>
	            <p class="answer">
	            We publish the crawled data as dump files via the <a href="/wardrobe">wardrobe</a> as gzipped, sorted, N-Triples and N-Quads, or as indexed and compressed <a href="http://www.rdfhdt.org" target="_blank">HDT</a> files
	            The <a href="/wardrobe">wardrobe</a> provides access to <a href="https://github.com/LinkedDataFragments/Server.js" target="_blank">Triple Pattern Fragment APIs</a> as well.
	            The provenance and VoiD meta-data is accessible via our <a href="/sparql">SPARQL endpoint</a></p>
            </div>
            <div class="faqItem">
	            <p class="question">Why do you publish this?</p>
	            <p class="answer">Using and finding Linked Data takes time and effort. We are not yet at a point where all available Linked Data is clean, standard and easy to use. Many datasets contain syntax errors, duplicates, or are difficult to find.
	            We offer one single download location for Linked Data, and publish the Linked Data in a consistent simple (sorted) N-Triple format, making it easy to use and compare datasets</p>
            </div>
            <div class="faqItem">
	            <p class="question">Why Gzipped N-Triples / N-Quads?</p>
	            <p class="answer">Compared to hosting the whole LOD cloud via a SPARQL endpoint, hosting gzipped N-Triples / N-Quads is easy and doable</p>
            </div>
            <div class="faqItem">
	            <p class="question">How do I <i>use</i> these files?</p>
	            <p class="answer">Depends on what you would like to do:
	            <div style='margin-left: 20px;'>
		           <dl>
					  <dt>Host as SPARQL endpoint</dt>
					  <dd>Simply download the data, and unzip it</dd>
					  <dt>Host as <a href="https://github.com/LinkedDataFragments/Server.js" target="_blank">Triple Pattern Fragment</a></dt>
                      <dd>Download the <a href="http://www.rdfhdt.org" target="_blank">HDT</a> file of a dataset, and use that as the TPF backend storage type.</dd>
					  <dt>Analyze the <i>complete</i> dataset</dt>
					  <dd>If you would like to analyze the triples using your own code, make sure you take advantage of the way we publish the data.
					  <ol>
	                    <li>The data is gzipped: you do not have to unpack everything to analyze it. Instead, you can stream it! This will save you a lot of memory usage</li>
	                    <li>The data is in one single N-Triple / N-Quad format: using this knowledge, you can easily write your own super-fast parser, as you don't have to consider the complete scope of the N-Triple / N-Quad specification. 
	                    And even better, <a href="https://github.com/LODLaundry/GettingStarted" target="_blank">we provide a couple of parser implementations for you!</a> (in NodeJs, Java, and Python).
	                    </ol>
					  
					  </dd>
					  <dt>Use/analyze a <i>part of</i> the dataset</dt>
                      <dd>Depending on what part you would like to use, we suggest you use the <a href="https://github.com/LinkedDataFragments/Server.js" target="_blank">Triple Pattern Fragment API</a>.
                      Via this API you are able to e.g. select only those triples where the predicate is <code>&lt;http://example.com/pred&gt;</code>.
                      If you encounter latency issues, or if you simply prefer to analyze your dataset on your computer locally, then consider downloading the <a href="http://www.rdfhdt.org" target="_blank">HDT</a> file and issue triple-pattern queries on the file directly on the command-line. 
                      </dd>
					</dl>
<!-- 		            You want to host this data as SPARQL?If you would like to use it in a triple-store, simply unzip the data.  -->
<!-- 		            However,  -->
	
<!-- 		            </p> -->
	   	            
	            </div>
            </div>
  
            
          </div>
        </div>
      </div>
  
    <!--
        Bootstrap core JavaScript
        Placed at the end of the document so the pages load faster
    -->
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="/bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="/bower_components/d3/d3.min.js"></script>
    <script src="/bower_components/YASR/dist/yasr.min.js"></script>
    <script src="/bower_components/YASQE/dist/yasqe.min.js"></script>
    <script src="/main.js"></script>
      <script>
     (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
     ga('create', analyticsId, 'auto');
     ga('send', 'pageview');
    
    </script>
  </body>
</html>

